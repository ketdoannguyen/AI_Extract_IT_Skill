{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0cec7a5",
   "metadata": {},
   "source": [
    "<H3 style=\"text-align:center;\">üçÄ IT skill NER Project using Spacy üçÄ</H3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d44e2a7",
   "metadata": {},
   "source": [
    "<b style=\"font-size :20px;\">Step 1 :</b>\n",
    "<span style=\"font-size :18px;\">Download the necessary modules and libraries üåæ</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "222a9f1b",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px;\">1. Check Python's version</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5c389703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: /home/vkuai/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Python 3.9.13\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44423dfe",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px;\">2. Install Pytorch for CUDA 11.6</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1178e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddaf3551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version : 2.0.0+cu118\n",
      "Is available : True\n",
      "Device count : 1\n",
      "Current device : 0\n",
      "Info device 0 : <torch.cuda.device object at 0x7fa9d045ebb0>\n",
      "Get device 0 name : NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Version :\", torch.__version__)\n",
    "print(\"Is available :\", torch.cuda.is_available())\n",
    "print(\"Device count :\", torch.cuda.device_count())\n",
    "print(\"Current device :\", torch.cuda.current_device())\n",
    "print(\"Info device 0 :\", torch.cuda.device(0))\n",
    "print(\"Get device 0 name :\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e05756ca",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px;\">3. Install spaCy with the extras for your CUDA version and transformers.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb540ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! export CUDA_PATH = \"/opt/nvidia/cuda\"\n",
    "! pip install -U spacy[cuda116, transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea12e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install -c conda-forge cupy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "407f522a",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px;\">4. Download the trained transformer-based pipelines by Spacy : en_core_web_trf</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d146a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08af366f",
   "metadata": {},
   "source": [
    "<b style=\"font-size :20px;\">Step 2 :</b></br>\n",
    "<span style=\"font-size :18px;\">From dictionary skill, use EntityRuler to automatically assign labels to provide training data IT NER model</H5>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ad07526",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px;\">1. Import pandas , spacy and spacy API libraries</span><br>\n",
    "<span style=\"font-size: 16px;\">Spacy API : language.Language, matcher.Matcher, tokens.Span, tokens.DocBin</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476e6c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "print(spacy.__version__)\n",
    "from spacy.tokens import Span\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.tokens.span import Span\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d84c054e",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px;\">2. Use GPU for spacy training IT skill NER model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac59292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Allocate data and perform operations on GPU. Will raise an error if no GPU is available.\n",
    "# Use the GPU, with memory allocations directed via PyTorch.\n",
    "# This prevents out-of-memory errors that would otherwise occur from competing\n",
    "# memory pools.\n",
    "from thinc.api import set_gpu_allocator, require_gpu, set_active_gpu\n",
    "#set_gpu_allocator(\"pytorch\")\n",
    "gpu = require_gpu(0)\n",
    "#set_active_gpu(0)\n",
    "gpu\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d1b750",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px;\">3. Load model en_core_web_trf , add pipeline \"entity_ruler\"</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a9a6349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba5a3b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'entity_ruler',\n",
       " 'ner']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_pattern_path = \"./data/EntityRuler_Patterns.jsonl\"\n",
    "\n",
    "if \"entity_ruler\" in nlp.pipe_names:\n",
    "    ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "else:\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "ruler.from_disk(skill_pattern_path)\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db733af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def create_config(model_name: str, component_to_update: str, output_path: Path):\n",
    "    nlp = spacy.load(model_name)\n",
    "\n",
    "    # create a new config as a copy of the loaded pipeline's config\n",
    "    config = nlp.config.copy()\n",
    "\n",
    "    # revert most training settings to the current defaults\n",
    "    default_config = spacy.blank(nlp.lang).config\n",
    "    config[\"corpora\"] = default_config[\"corpora\"]\n",
    "    config[\"training\"][\"logger\"] = default_config[\"training\"][\"logger\"]\n",
    "\n",
    "    # copy tokenizer and vocab settings from the base model, which includes\n",
    "    # lookups (lexeme_norm) and vectors, so they don't need to be copied or\n",
    "    # initialized separately\n",
    "    config[\"initialize\"][\"before_init\"] = {\n",
    "        \"@callbacks\": \"spacy.copy_from_base_model.v1\",\n",
    "        \"tokenizer\": model_name,\n",
    "        \"vocab\": model_name,\n",
    "    }\n",
    "    config[\"initialize\"][\"lookups\"] = None\n",
    "    config[\"initialize\"][\"vectors\"] = None\n",
    "\n",
    "    # source all components from the loaded pipeline and freeze all except the\n",
    "    # component to update; replace the listener for the component that is\n",
    "    # being updated so that it can be updated independently\n",
    "    config[\"training\"][\"frozen_components\"] = []\n",
    "    for pipe_name in nlp.component_names:\n",
    "        if pipe_name != component_to_update:\n",
    "            config[\"components\"][pipe_name] = {\"source\": model_name}\n",
    "            config[\"training\"][\"frozen_components\"].append(pipe_name)\n",
    "        else:\n",
    "            config[\"components\"][pipe_name] = {\n",
    "                \"source\": model_name,\n",
    "                \"replace_listeners\": [\"model.tok2vec\"],\n",
    "            }\n",
    "\n",
    "    # save the config\n",
    "    config.to_disk(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d093ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_config(\"en_core_web_trf\",\"ner\",'./ner_config.cfg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe5290f3",
   "metadata": {},
   "source": [
    "<b style=\"font-size :20px;\">Step 3 :</b></br>\n",
    "<span style=\"font-size :18px;\">PoS Tagging , Noun Phrase , Combination Skills</H5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ex = [\"Senior\",\"Junior\",\"Trainee\",\"Internship\",\"Fresher\",\"Sr\",\"Jr\"]\n",
    "skill_phrase = [\"software\",\"algorithm\",\"library\", \"model\", \"tool\",\"module\",\"platform\",\"method\",\"equipment\",\"component\",\"testing\",\"engine\",\n",
    "                \"management\", \"development\", \"methodology\", \"certify\", \"certification\",\"programming\",\"analysis\",\"application\",\"analysis\",\"research\",\n",
    "                \"technology\", \"technical\", \"technique\", \"language\", \"infrastructure\", \"design\", \"system\",\"systems\",\"network\",\"networks\",\"web\",\"code\",\"process\"]\n",
    "word_ing = [\"overseeing\",\"developing\",\"transforming\",\"building\",\"implementing\",\"analyzing\",\"executing\",\"writing\",\"deploying\",\"troubleshooting\",\n",
    "                \"managing\",\"configuring\",\"designing\",\"maintaining\",\"evaluating\",\"running\",\"monitoring\",\"solving\",\"resolving\",\"leveraging\",\"standardizing\",\n",
    "                \"operating\",\"establishing\",\"integrating\",\"optimizing\",\"coding\",\"learning\",\"updating\",\"securing\",\"fixing\",\"training\",\"testing\",\n",
    "                \"utilizing\",\"recommending\",\"producing\",\"defining\",\"ng\",\"mining\",\"creating\",\"supporting\"]\n",
    "word_after_remove = [\"year\",\"years\",\"experience\",\"knowledge\",\"excellent\"]\n",
    "word_ing_remove = [\"including\",\"seeking\",\"using\",\"looking\",\"following\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8115b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ing_rules(doc:Doc, idx: int) -> int:\n",
    "    \"\"\"\n",
    "    Input:  doc: Doc \n",
    "            token: Token -> token is the verb in front of the noun phrase, in word_ing list\n",
    "    Return: Number of tokens that satisfy the verb rule ing suffix after the noun phrase\n",
    "    \"\"\"\n",
    "    t = 1\n",
    "    remove = [\"including\",\"seeking\",\"using\",\"looking\"]\n",
    "    if (idx-t) < 0:\n",
    "        return 0\n",
    "    while True :\n",
    "        if (idx-t) > 0 and re.search(\"ing$\",doc[idx-t].text) and doc[idx-t].text not in remove: \n",
    "            t += 1\n",
    "        elif (idx-t-1) > 0 and doc[idx-t].text in [\"and\",\"or\",\",\",\"/\",\"&\",\"and/or\"] and re.search(\"ing$\",doc[idx-t-1].text):\n",
    "            t += 2\n",
    "        elif (idx-t-2) > 0 and doc[idx-t].text in [\"and\",\"or\"] and doc[idx-t-1].text == \",\" and re.search(\"ing$\",doc[idx-t-2].text):\n",
    "            t += 3\n",
    "        else : \n",
    "            return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d061d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token_to_add(token: Token, \n",
    "                         skill_phrase: list, \n",
    "                         word_ing: list, \n",
    "                         index_ex: list, \n",
    "                         token_in_noun_phrase: list) -> bool:\n",
    "    if token.pos_ in [\"NOUN\",\"PROPN\"] :\n",
    "        return True\n",
    "    if token.ent_type_ == \"HARD-SKILL\" :\n",
    "        return True\n",
    "    if token.lemma_ in skill_phrase :\n",
    "        return True\n",
    "    if token.lower_ in word_ing :\n",
    "        return True\n",
    "    if token.text in index_ex :\n",
    "        return True\n",
    "    if token in token_in_noun_phrase:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08dd5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token_break(token: Token) -> bool:\n",
    "    if token.pos_ in [\"PRON\",\"DET\",\"ADV\",\"CCONJ\",\"NUM\"]:\n",
    "        return True\n",
    "    elif (not re.search(\"[a-zA-Z]\",token.text)) :\n",
    "        return True\n",
    "    elif token.lower_ in [\"strong\",\"existing\",\"experience\",\"knowledge\",\"excellent\",\"new\",\"other\",\"that\",\"full\",\"e.g.\",\"good\",\"necessary\",\"and\",\"or\",\"&\",\"year\",\"years\"]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e28fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_dash(doc:Doc, index : int)-> bool:\n",
    "    if index>0 and index<len(doc) and doc[index].text == \"-\":\n",
    "        if doc.text[doc[index].idx-1] != \" \" and doc.text[doc[index].idx+1] != \" \" :\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357c1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_roman_numeral(numeral):\n",
    "    numeral = {c for c in numeral}\n",
    "    validRomanNumerals = {c for c in \"XVI\"}\n",
    "    return not numeral - validRomanNumerals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61cdcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary_word_in_chunk(doc: Doc, chunk: Span, word_ing: bool = True) -> Span:\n",
    "    \"\"\"\n",
    "    Remove unneccessary words in chunk\n",
    "    Words have pos [\"PRON\",\"DET\",\"ADV\",\"CCONJ\",\"NUM\"], without letters and some exception words\n",
    "\n",
    "    Input:  chunk: Span -> 1 noun phrase in doc.noun_chunks\n",
    "    Return: Span -> Span removed all unneccessary words\n",
    "    \"\"\"\n",
    "    len_chunk = len(chunk)\n",
    "    start_char = chunk.start_char\n",
    "    pre = 0\n",
    "    while True :\n",
    "        if len_chunk == 0 :\n",
    "            break\n",
    "        elif check_token_break(chunk[pre]):\n",
    "            if chunk[pre].i < (len(doc)-1) and doc.text[start_char + len(chunk[pre].text)] != \" \":\n",
    "                start_char = start_char + len(chunk[pre].text)\n",
    "            else:\n",
    "                start_char = start_char + len(chunk[pre].text) + 1\n",
    "            pre += 1\n",
    "            len_chunk -= 1 \n",
    "        else:\n",
    "            break\n",
    "    span = doc.char_span(start_char,chunk.end_char)\n",
    "    if word_ing :\n",
    "        if (len_chunk == 1 and span[0].pos_ not in [\"NOUN\",\"PROPN\"]) or len_chunk == 0 :\n",
    "            return None\n",
    "    else:\n",
    "        if (len_chunk == 1 and (not re.search(\"A-Z\",span.text))) or len_chunk == 0 :\n",
    "            return None\n",
    "    return span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b42456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tokens_in_noun_chunks(doc: Doc)-> list:\n",
    "    \"\"\"\n",
    "    Remove unnecessary words in each chunk\n",
    "    Then, get all tokens in nouns_chunk\n",
    "\n",
    "    Input:  doc: Doc\n",
    "    Return: list -> list contain all tokens in nouns_chunk\n",
    "    \"\"\"\n",
    "    token_list = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        span = remove_unnecessary_word_in_chunk(doc,chunk)\n",
    "        if span != None :\n",
    "            for token in span :\n",
    "                token_list.append(token)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb97b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_element_duplicate(list_entities:list) -> list:\n",
    "    # remove duplicate elements in list\n",
    "    list_entities = list(set(list_entities))\n",
    "    list_entities = sorted(list_entities, key=lambda a: a[0])\n",
    "\n",
    "    # if len(list_ents)  <= 1 then no need to check\n",
    "    if len(list_entities) > 1:\n",
    "        list_remove_ent = []\n",
    "        # iterate each element in array list_ents\n",
    "        for tuple_ent in list_entities:\n",
    "            # check each element in the array against all other elements in the array\n",
    "            for check in list_entities:\n",
    "                if check != tuple_ent:\n",
    "                    if tuple_ent[0] >= check[0] and tuple_ent[1] <= check[1]:\n",
    "                        list_remove_ent.append(tuple_ent)\n",
    "                        break\n",
    "                \n",
    "        list_entities = [ent for ent in list_entities if ent not in list_remove_ent]\n",
    "\n",
    "    if len(list_entities) > 1:\n",
    "        list_remove_ent = []\n",
    "        list_add_ent = []\n",
    "        # iterate each element in array list_ents\n",
    "        for tuple_ent in list_entities:\n",
    "            # check each element in the array against all other elements in the array\n",
    "            for check in list_entities:\n",
    "                if check != tuple_ent:\n",
    "                    if tuple_ent[0] <= check[0] and check[0] <= tuple_ent[1] and tuple_ent[1] <= check[1]:\n",
    "                        tuple_ent_new = (tuple_ent[0], check[1], \"HARD-SKILL\")\n",
    "                        list_add_ent.append(tuple_ent_new)\n",
    "                        list_remove_ent.append(tuple_ent)\n",
    "                        list_remove_ent.append(check)\n",
    "\n",
    "        list_entities = [ent for ent in list_entities if ent not in list_remove_ent]\n",
    "        for ent in list_add_ent:\n",
    "            list_entities.append(ent)\n",
    "    return list_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d58d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_other_ents(doc:Doc, list_entities:list, is_comma_rule:bool) -> list:\n",
    "    list_label = [\"ORG\", \"LANGUAGE\", \"GPE\",\"TIME\", \"MONEY\"]\n",
    "    if is_comma_rule :\n",
    "        list_label.append(\"HARD-SKILL\")\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in list_label:\n",
    "            check = True\n",
    "            for start_char, end_char, _ in list_entities:\n",
    "                for i in range(ent.start_char, ent.end_char):\n",
    "                    if i >= start_char and i <= end_char:\n",
    "                        check = False\n",
    "                        break\n",
    "            if check:\n",
    "                tuplee = (ent.start_char, ent.end_char, ent.label_)\n",
    "                list_entities.append(tuplee)\n",
    "    list_entities = sorted(list_entities, key=lambda a: a[0])\n",
    "    return list_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_pre_token(doc:Doc,token:Token,token_noun_chunks:list) -> int:\n",
    "    i = 0\n",
    "    if rule_dash(doc,index=token.i-1):\n",
    "        i = 3\n",
    "    elif rule_dash(doc,index=token.i):\n",
    "        i = 2\n",
    "    elif re.search(\"ing$\",token.text) \\\n",
    "        and token.pos_ == \"VERB\" \\\n",
    "        and token.text not in word_ing_remove:\n",
    "        i = ing_rules(doc = doc, idx = token.i)\n",
    "    elif check_token_to_add(token,token_noun_chunks) \\\n",
    "        and token.lower_ not in word_after_remove:\n",
    "        i = 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_after_token(doc:Doc,token:Token,token_noun_chunks:list) -> int:\n",
    "    i = 0\n",
    "    if rule_dash(doc,index=token.i+1):\n",
    "        i = 3\n",
    "    elif rule_dash(doc,index=token.i):\n",
    "        i = 2\n",
    "    elif token.text == \"(\" \\\n",
    "        and re.search(\"[A-Z]\",doc[token.i + 1].text) \\\n",
    "        and doc[token.i + 1].lower_ not in [\"required\",\"preferred\"] \\\n",
    "        and doc[token.i + 2].text == \")\" :\n",
    "        i = 3\n",
    "    elif check_if_roman_numeral(token.text):\n",
    "        i = 1\n",
    "    elif (token.like_num and (not re.search(\"[a-zA-Z]\",token.text))):\n",
    "        if (token.i+1)<len(doc) and doc[token.i+1].text == \"+\" :\n",
    "            i = 2\n",
    "        else:\n",
    "            i = 1\n",
    "    elif check_token_to_add(token,token_noun_chunks)\\\n",
    "        and token.lower_ not in word_after_remove:\n",
    "        i = 1\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0615a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_ents(doc:Doc) -> list:\n",
    "    list_entities = []\n",
    "\n",
    "    # rule _ing (*)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        span = None\n",
    "        if doc[chunk[0].i-1].lower_ in word_ing :\n",
    "            span = remove_unnecessary_word_in_chunk(doc,chunk,word_ing=True)\n",
    "            if span != None :\n",
    "                t = ing_rules(doc = doc, idx = chunk[0].i-1)\n",
    "                span = doc.char_span(doc[chunk[0].i-t].idx, chunk.end_char)\n",
    "        elif doc[chunk[0].i-1].lower_ in word_ing_remove:\n",
    "            span = remove_unnecessary_word_in_chunk(doc,chunk,word_ing=False)\n",
    "        if span != None :\n",
    "            tuplee = (span.start_char, span.end_char, \"HARD-SKILL\")\n",
    "            list_entities.append(tuplee)\n",
    "    \n",
    "    token_noun_chunks = list_tokens_in_noun_chunks(doc)\n",
    "    for token in doc:\n",
    "        if  token.ent_type_ == \"HARD-SKILL\" \\\n",
    "            or token.lemma_.lower() in skill_phrase \\\n",
    "            or token.text in index_ex : \n",
    "            pre = 1\n",
    "            after = 1\n",
    "            while True:\n",
    "                if token.i - pre < 0:\n",
    "                    break\n",
    "                number = repeat_pre_token(doc,doc[token.i - pre],token_noun_chunks)\n",
    "                if number != 0 and doc[token.i - pre].text != \".\":\n",
    "                    pre += number\n",
    "                else:\n",
    "                    break\n",
    "            while True:\n",
    "                if token.i + after >= len(doc):\n",
    "                    break\n",
    "                number = repeat_after_token(doc,doc[token.i + after],token_noun_chunks)\n",
    "                if number != 0 and doc[token.i + after].text != \".\":\n",
    "                    after += number\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "            span = Span(doc, token.i - pre + 1, token.i + after, label=\"HARD-SKILL\")\n",
    "            if len(span) == 1 and span[0].ent_type_ != \"HARD-SKILL\":\n",
    "                pass\n",
    "            else:\n",
    "                tuplee = (span.start_char, span.end_char, \"HARD-SKILL\")\n",
    "                list_entities.append(tuplee)\n",
    "            \n",
    "    list_entities = remove_element_duplicate(list_entities)\n",
    "    list_entities = add_other_ents(doc,list_entities,is_comma_rule=False)\n",
    "    return list_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f45272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_ents_into_doc(doc:Doc,list_ents:list):\n",
    "    ents_new = []\n",
    "    for start,end,label in list_ents:\n",
    "        span = doc.char_span(start,end,label)\n",
    "        ents_new.append(span)\n",
    "    if len(ents_new)>0:\n",
    "        doc.ents = ents_new\n",
    "    else:\n",
    "        doc.ents = []\n",
    "    return doc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1978bf6e",
   "metadata": {},
   "source": [
    "<b style=\"font-size :20px;\">Step 4 :</b></br>\n",
    "<span style=\"font-size :18px;\">Read dataset JobDescription.csv</H5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1880f188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28334 entries, 0 to 28333\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   JD      28334 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 221.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/JobDescription.csv\", sep=\"\\t\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43abb50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning / AI Internship. Summary: As ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior AI/ML Engineer. We are seeking a dedica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Artificial Intelligence (AI) / Machine Learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine Learning Engineer. Botkeeper is an aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI/ML engineer (Artificial intelligence / Mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Artificial Intelligence Developer. Stillwater ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vision Machine Learning Engineer - US Remote. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Machine Learning Engineer - US Remote. Descrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Machine Learning Researcher. A successful star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Python, Digita: Machine Learning, Digita: Arti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  JD\n",
       "0  Machine Learning / AI Internship. Summary: As ...\n",
       "1  Junior AI/ML Engineer. We are seeking a dedica...\n",
       "2  Artificial Intelligence (AI) / Machine Learnin...\n",
       "3  Machine Learning Engineer. Botkeeper is an aut...\n",
       "4  AI/ML engineer (Artificial intelligence / Mach...\n",
       "5  Artificial Intelligence Developer. Stillwater ...\n",
       "6  Vision Machine Learning Engineer - US Remote. ...\n",
       "7  Machine Learning Engineer - US Remote. Descrip...\n",
       "8  Machine Learning Researcher. A successful star...\n",
       "9  Python, Digita: Machine Learning, Digita: Arti..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b51b469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI/ML Systems Lead. Full Job Description: The Position. Who We Are: Our Strategic Analytics & Intelligence (SAI) team isn't just deciphering data. We're here to help solve the world's most complex healthcare challenges and improve the lives of patients. With a mix of competitive intelligence, market research, data science, advanced analytics, access, and forecasting, SAI unlocks key insights for our internal partners that ultimately benefit healthcare providers and patients. Even if you've never worked in biotech, you'll establish yourself as an expert alongside other specialists. Plus, you can gain new experiences across marketing disciplines, therapeutic areas, and commercial operations. The entire time, you'll be surrounded by a diverse and inclusive team that aims to reflect the world we serve. The data science team within SAI exists to help the CMG (Commercial, Medical and Government) organization achieve its vision by unlocking value from data quicker and more effectively. As a center of excellence, the data science team has 3 primary remits: Owning Advanced Data Science Strategy and Innovation for SAI and CMG; Application of Data Science to CMG enterprise priorities, and; Driving the Data Science Capability on behalf of the SAI department. The team leverages advanced data science capabilities, working across CMG to develop strategic and holistic solutions by identifying and leading innovative analytic projects & pilots to enable GNE to deliver for our customers. What We Are Looking for We are looking for an experienced AI/ML Systems Lead to lead the design and implementation of cutting-edge AI/ML solutions for a growing organization. If you have a passion for innovation and a proven track record in designing and implementing AI/ML systems, this role is made for you. As the Lead AI/ML Systems Lead, you will be responsible for designing and implementing Artificial Intelligence and Machine Learning (AI/ML) systems to support business objectives. You will work closely with the software development and data science teams to design, build, and implement AI/ML models that are accurate, efficient, scalable, and secure. Your role will involve working with stakeholders to identify business needs and requirements, proposing technical solutions, and leading the implementation of the chosen solutions. Responsibilities: Work closely with business stakeholders to identify business needs and requirements, and propose technical solutions that leverage AI/ML technologies. Develop and maintain a comprehensive understanding of the organization's technical landscape, data infrastructure, and data processing pipelines. Lead the design and implementation of AI/ML systems that are accurate, efficient, scalable, and secure. Work with the data science team to identify the appropriate algorithms and models for specific business problems, and implement them in production systems. Work with the software development team to integrate AI/ML systems with existing business applications and data systems. Develop and maintain documentation for AI/ML systems, including technical specifications, system architecture diagrams, and user guides. Ensure that AI/ML systems comply with relevant regulatory requirements, such as data privacy laws and industry standards. Keep up to date with emerging AI/ML technologies, and assess their potential impact on the organization's technical landscape. Requirements: Bachelor's or Master's degree in Computer Science, Engineering, Mathematics, or a related field and 10+ years of relevant work experience or An equivalent mix of education and work experience is required. 5+ years of experience in designing and implementing AI/ML systems in a commercial environment. Strong knowledge of AI/ML technologies, including deep learning, natural language processing, and computer vision. Strong knowledge of programming languages such as Python, R, Julia, etc. Strong knowledge of data processing and storage technologies such as Hadoop, Spark, and NoSQL databases. Experience with cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Excellent problem-solving and analytical skills, with the ability to identify and solve complex technical problems. Excellent communication and collaboration skills, with the ability to work effectively with stakeholders across the organization. Strong leadership skills, with the ability to lead a team of technical professionals. This position is located in South San Francisco and works from the office a majority of the time. Relocation benefits are available. The expected salary range for this position based on the primary location of California is $178,300 - $331,100. Actual pay will be determined based on experience, qualifications, geographic location, and other job-related factors permitted by law. A discretionary annual bonus may be available based on individual and Company performance. This position also qualifies for the benefits detailed at the link provided below.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "list_JDs = list(df[\"JD\"])\n",
    "random.shuffle(list_JDs)\n",
    "list_JDs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eabe86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3010.3756617491354"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(job) for job in list_JDs)/len(list_JDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04158336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for jd in list_JDs[:1000]:\n",
    "    doc = nlp(jd)\n",
    "    \n",
    "           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b55ca067",
   "metadata": {},
   "source": [
    "<b style=\"font-size :20px;\">Step 5 :</b></br>\n",
    "<span style=\"font-size :18px;\">Testing rules</H5>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "478a90a8",
   "metadata": {},
   "source": [
    "<b style=\"font-size :18px;\">Testing _ing rule</b></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for jd in list_JDs[:1000]:\n",
    "    doc = nlp(jd)\n",
    "    # rule _ing\n",
    "    for chunk in doc.noun_chunks:\n",
    "        span = None\n",
    "        if doc[chunk[0].i-1].lower_ in word_ing :\n",
    "            span = remove_unnecessary_word_in_chunk(doc,chunk,word_ing=True)\n",
    "            if span != None :\n",
    "                t = ing_rules(doc = doc, idx = chunk[0].i-1)\n",
    "                span = doc.char_span(doc[chunk[0].i-t].idx, chunk.end_char)\n",
    "        elif doc[chunk[0].i-1].lower_ in [\"using\",\"seeking\",\"looking\"]:\n",
    "            span = remove_unnecessary_word_in_chunk(doc,chunk,word_ing=False)\n",
    "        if span != None :\n",
    "            print(span)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e7e663b",
   "metadata": {},
   "source": [
    "<b style=\"font-size :18px;\">Testing dash rule</b></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "204210c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(is_rule_dash=False,is_rule_parenthesis=False,is_rule_link_num=False):\n",
    "    for jd in list_JDs[:1000]:\n",
    "        doc = nlp(jd)\n",
    "        token_noun_chunks = list_tokens_in_noun_chunks(doc)\n",
    "        for token in doc:\n",
    "            check_rule_dash = False\n",
    "            check_rule_parenthesis = False\n",
    "            check_link_num = False\n",
    "            if  token.ent_type_ == \"HARD-SKILL\" \\\n",
    "                or token.lemma_.lower() in skill_phrase \\\n",
    "                or token.text in index_ex : \n",
    "                pre = 1\n",
    "                after = 1\n",
    "                while True:\n",
    "                    if token.i - pre >= 0:\n",
    "                        pre_token = doc[token.i - pre]\n",
    "                        if rule_dash(doc,index=token.i-pre-1):\n",
    "                            pre += 3\n",
    "                            check_rule_dash = True\n",
    "                        elif rule_dash(doc,index=token.i-pre):\n",
    "                            pre += 2\n",
    "                            check_rule_dash = True\n",
    "                        elif re.search(\"ing$\",pre_token.text) \\\n",
    "                            and pre_token.pos_ == \"VERB\" \\\n",
    "                            and pre_token.text not in [\"including\",\"seeking\",\"using\"]:\n",
    "                            pre += ing_rules(doc = doc, idx = pre_token.i)\n",
    "                        elif (pre_token.pos_ in [\"NOUN\",\"PROPN\"] \\\n",
    "                            or check_token_to_add(pre_token,skill_phrase,word_ing,index_ex,token_noun_chunks)) \\\n",
    "                            and pre_token.lower_ not in word_after_remove:\n",
    "                            pre += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                while True:\n",
    "                    if token.i + after < len(doc):\n",
    "                        after_token = doc[token.i + after]\n",
    "                        if rule_dash(doc,index=token.i+after+1):\n",
    "                            after += 3\n",
    "                            check_rule_dash = True\n",
    "                        elif rule_dash(doc,index=token.i+after):\n",
    "                            after += 2\n",
    "                            check_rule_dash = True\n",
    "                        elif after_token.text == \"(\":\n",
    "                            if  re.search(\"[A-Z]\",doc[token.i + after + 1].text) and \\\n",
    "                                doc[token.i + after + 1].lower_ not in [\"required\",\"preferred\"] and \\\n",
    "                                doc[token.i + after + 2].text == \")\" :\n",
    "                                after += 3\n",
    "                                check_rule_parenthesis = True\n",
    "                            else:\n",
    "                                break\n",
    "                        elif checkIfRomanNumeral(after_token.text):\n",
    "                            after += 1\n",
    "                            check_link_num = True\n",
    "                        elif (after_token.like_num and (not re.search(\"[a-zA-Z]\",after_token.text))):\n",
    "                            if (token.i+after+1)<len(doc) and doc[token.i+after+1].text == \"+\" :\n",
    "                                after += 1\n",
    "                            after += 1\n",
    "                            check_link_num = True\n",
    "                        elif (after_token.pos_ in [\"NOUN\",\"PROPN\"] \\\n",
    "                            or check_token_to_add(after_token,skill_phrase,word_ing,index_ex,token_noun_chunks))\\\n",
    "                                and after_token.lower_ not in word_after_remove:\n",
    "                            after += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                span = Span(doc, token.i - pre + 1, token.i + after, label=\"HARD-SKILL\")\n",
    "                if len(span) == 1 and span[0].ent_type_ != \"HARD-SKILL\":\n",
    "                    pass\n",
    "                else:\n",
    "                    if is_rule_dash and check_rule_dash:\n",
    "                        print(span)\n",
    "                    elif is_rule_parenthesis and check_rule_parenthesis:\n",
    "                        print(span)\n",
    "                    elif is_rule_link_num and check_link_num:\n",
    "                        print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59909558",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(is_rule_dash=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db759f6e",
   "metadata": {},
   "source": [
    "<b style=\"font-size :18px;\">Testing parenthesis rule</b></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82613775",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(is_rule_parenthesis=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6802c52",
   "metadata": {},
   "source": [
    "<b style=\"font-size :18px;\">Testing number rule</b></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f77bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(is_rule_link_num=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47a9876f",
   "metadata": {},
   "source": [
    "<b style=\"font-size :20px;\">Step 6 :</b></br>\n",
    "<span style=\"font-size :18px;\">Testing accuracy on a job description</H5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(list_JDs[0])\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ents = []\n",
    "for start,end,label in get_list_ents(doc):\n",
    "    span = doc.char_span(start,end,label)\n",
    "    list_ents.append(span)\n",
    "doc.ents = list_ents\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ab6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(list_JDs[0])\n",
    "print(doc)\n",
    "list_ents = get_list_ents(doc)\n",
    "print(\"Total : \"+str(len(list_ents))+\" ents\")\n",
    "print(\"-\"*70)\n",
    "for start, end, label in list_ents:\n",
    "    print(\"{:45} | {:5} | {:5} | {:8}\".format(\n",
    "        doc.char_span(start, end).text, str(start), str(end), label))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0750399c",
   "metadata": {},
   "source": [
    "<b style=\"font-size :20px;\">Step 7 :</b></br>\n",
    "<span style=\"font-size :18px;\">Creating train.spacy and dev.spacy </H5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f70f54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñâ        | 5042/25500 [12:53<1:06:42,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñâ        | 5043/25500 [13:17<36:05:39,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 8398/25500 [21:54<39:07,  7.29it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 8400/25500 [22:18<19:29:30,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 14633/25500 [38:31<20:49,  8.70it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n",
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 14635/25500 [38:57<16:08:48,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14950/25500 [39:45<19:01,  9.24it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14950/25500 [40:01<19:01,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 14954/25500 [40:21<10:38:05,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 15646/25500 [42:05<18:59,  8.65it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 16048/25500 [43:34<5:50:01,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 21078/25500 [56:30<2:32:52,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 21723/25500 [58:00<07:18,  8.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bug\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 22277/25500 [59:33<1:51:20,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25500/25500 [1:07:33<00:00,  6.29it/s]\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = list()\n",
    "for job_des in tqdm(list_JDs[:25500]):\n",
    "    try:\n",
    "        doc = nlp(job_des)\n",
    "        list_ents = get_list_ents(doc)\n",
    "        TRAINING_DATA.append((doc.text, {\"entities\": list_ents}))\n",
    "    except:\n",
    "       print(\"bug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd577a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 122/2834 [00:28<1:27:52,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1795/2834 [04:46<35:11,  2.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2834/2834 [07:17<00:00,  6.48it/s]\n"
     ]
    }
   ],
   "source": [
    "DEV_DATA = list()\n",
    "for job_des in tqdm(list_JDs[25500:]):\n",
    "    try:\n",
    "        doc = nlp(job_des)\n",
    "        list_ents = get_list_ents(doc)\n",
    "        DEV_DATA.append((doc.text, {\"entities\": list_ents}))\n",
    "    except:\n",
    "        print(\"bug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "107e665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25499/25499 [00:32<00:00, 776.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from spacy.training import Example\n",
    "db = DocBin()\n",
    "for text, entities in tqdm(TRAINING_DATA):\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, entities)\n",
    "    db.add(example.reference)\n",
    "\n",
    "db.to_disk(\"./config/train.spacy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59562730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2834/2834 [00:03<00:00, 778.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from spacy.training import Example\n",
    "db = DocBin()\n",
    "for text, entities in tqdm(DEV_DATA):\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, entities)\n",
    "    db.add(example.reference)\n",
    "\n",
    "db.to_disk(\"./config/dev.spacy\")  # save the docbin object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc35e8d4",
   "metadata": {},
   "source": [
    "<b style=\"font-size :20px;\">Step 8 :</b></br>\n",
    "<span style=\"font-size :18px;\">Training model IT_skill_NER with train.spacy and dev.spacy </H5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e62939",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config ./config/base_config.cfg ./config/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a57b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy debug config ./config/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy debug data ./config/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init labels ./config/config.cfg ./config/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ./config/config.cfg --output ./IT_skill_NER --gpu-id 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4ea0bf9",
   "metadata": {},
   "source": [
    "<b style=\"font-size :20px;\">Step 9 :</b></br>\n",
    "<span style=\"font-size :18px;\">Load IT_skill_NER model</H5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e45af99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer', 'ner']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"./IT_skill_NER/model-best\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "647db101",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45d2b2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GPE', 'HARD-SKILL', 'LANGUAGE', 'MONEY', 'ORG', 'TIME')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "790c885e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_JDs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_175103/1890743356.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_JDs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjupyter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_JDs' is not defined"
     ]
    }
   ],
   "source": [
    "doc = nlp(list_JDs[17])\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.dep_ for token in doc[:10]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2640e6d7",
   "metadata": {},
   "source": [
    "<b style=\"font-size :20px;\">Step 10 :</b></br>\n",
    "<span style=\"font-size :18px;\">CommaRule</H5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b07f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Computer science, software engineering, various operating systems, information security fundamentals or general IT, and procedures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af4400ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comma_rule_token_len_1_2(doc:Doc, list_entities:list,count:int, start:int, end:int)-> list:\n",
    "    is_break = False\n",
    "    if start == end :\n",
    "        return list_entities,is_break\n",
    "    if count == 1 and doc[start].lower_ not in word_ing_remove:\n",
    "        span = Span(doc,start, end, label=\"HARD-SKILL\")\n",
    "        if re.search(\"A-Z\",span.text):\n",
    "            tuplee = (span.start_char,span.end_char,\"HARD-SKILL\")\n",
    "            list_entities.append(tuplee)\n",
    "    elif count == 2:\n",
    "        if doc[start].lower_ not in word_ing_remove:\n",
    "            span = Span(doc,start, end, label=\"HARD-SKILL\")\n",
    "            tuplee = (span.start_char,span.end_char,\"HARD-SKILL\")\n",
    "            list_entities.append(tuplee)\n",
    "        else :\n",
    "            if start+1 != end:\n",
    "                span = Span(doc,start+1, end, label=\"HARD-SKILL\")\n",
    "                tuplee = (span.start_char,span.end_char,\"HARD-SKILL\")\n",
    "                list_entities.append(tuplee)\n",
    "            is_break = True\n",
    "    return list_entities,is_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc972c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comma_rule(doc:Doc):\n",
    "    doc = append_ents_into_doc(doc,get_list_ents(doc))\n",
    "    word = [\"and\",\"or\",\",\",\"/\",\"&\",\"and/or\"]\n",
    "    token_noun_chunks = list_tokens_in_noun_chunks(doc)\n",
    "    list_entities = [] \n",
    "    for ent in doc.ents :\n",
    "        if ent.label_ == \"HARD-SKILL\":\n",
    "            if doc[ent.start-1].text in word:\n",
    "                count = 0\n",
    "                step_pre = 0\n",
    "                step = 0\n",
    "                check_exit = False\n",
    "                while True :\n",
    "                    if ent.start-step >= 0 :\n",
    "                        break\n",
    "                    if doc[ent.start-step].text in word or count == 8 or doc[ent.start-step].text == \".\":\n",
    "                        if count != 0 :\n",
    "                            start = ent.start-step+1\n",
    "                            end = ent.start-step_pre\n",
    "                            if count <= 2 :\n",
    "                                list_entities,is_break = comma_rule_token_len_1_2(doc,list_entities,count,start,end)\n",
    "                                if is_break:\n",
    "                                    break\n",
    "                            else :\n",
    "                                i = end - 1\n",
    "                                while i >= start :\n",
    "                                    number = repeat_pre_token(doc,doc[i],token_noun_chunks)\n",
    "                                    if number != 0:\n",
    "                                        i -= number\n",
    "                                    else :\n",
    "                                        if (i+1) != end :\n",
    "                                            span = Span(doc,(i+1), end, label=\"HARD-SKILL\")\n",
    "                                            if len(span) == 1 :\n",
    "                                                if re.search(\"A-Z\",span.text):\n",
    "                                                    tuplee = (span.start_char,span.end_char,\"HARD-SKILL\")\n",
    "                                                    list_entities.append(tuplee)\n",
    "                                            else:\n",
    "                                                tuplee = (span.start_char,span.end_char,\"HARD-SKILL\")\n",
    "                                                list_entities.append(tuplee)\n",
    "                                        check_exit = True\n",
    "                                        break \n",
    "                            if count == 8 or doc[ent.start-step].text == \".\" or check_exit:\n",
    "                                break   \n",
    "                        step_pre = step\n",
    "                        count = 0\n",
    "                    else :\n",
    "                        count += 1\n",
    "                    step += 1\n",
    "                    \n",
    "            if doc[ent.end].text in word:\n",
    "                count = 0\n",
    "                step_pre = 0\n",
    "                step = 0\n",
    "                check_exit = False\n",
    "                while True :\n",
    "                    if ent.end+step < len(doc) :\n",
    "                        break\n",
    "                    if doc[ent.end+step].text in word or count == 8 or doc[ent.end+step].text == \".\":\n",
    "                        if count != 0 :\n",
    "                            start = ent.end+step_pre+1\n",
    "                            end = ent.end+step\n",
    "                            if count <= 2 :\n",
    "                                list_entities,is_break = comma_rule_token_len_1_2(doc,list_entities,count,start,end)\n",
    "                                if is_break:\n",
    "                                    break\n",
    "                            else :\n",
    "                                i = start\n",
    "                                while i < end :\n",
    "                                    number = repeat_after_token(doc,doc[i],token_noun_chunks)\n",
    "                                    if number != 0 :\n",
    "                                        i += number\n",
    "                                    else:\n",
    "                                        if i != start:\n",
    "                                            span = Span(doc,start, i, label=\"HARD-SKILL\")\n",
    "                                            if len(span) == 1 :\n",
    "                                                if re.search(\"A-Z\",span.text):\n",
    "                                                    tuplee = (span.start_char,span.end_char,\"HARD-SKILL\")\n",
    "                                                    list_entities.append(tuplee)\n",
    "                                            else:\n",
    "                                                tuplee = (span.start_char,span.end_char,\"HARD-SKILL\")\n",
    "                                                list_entities.append(tuplee)\n",
    "                                        check_exit = True\n",
    "                                        break \n",
    "        \n",
    "                            if count == 8 or doc[ent.end+step].text == \".\" or check_exit:\n",
    "                                break   \n",
    "                        step_pre = step\n",
    "                        count = 0\n",
    "                    else :\n",
    "                        count += 1\n",
    "                    step += 1\n",
    "\n",
    "    list_entities = remove_element_duplicate(list_entities)\n",
    "    list_entities = add_other_ents(doc,list_entities,is_comma_rule=True)\n",
    "    return list_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98855629",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'append_ents_into_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_175103/1758400065.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_ents_into_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomma_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'append_ents_into_doc' is not defined"
     ]
    }
   ],
   "source": [
    "doc = append_ents_into_doc(doc,comma_rule(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a864756e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Computer science\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">HARD-SKILL</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    software engineering\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">HARD-SKILL</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    various operating systems\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">HARD-SKILL</span>\n",
       "</mark>\n",
       ", information security fundamentals or general IT, and procedures.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
